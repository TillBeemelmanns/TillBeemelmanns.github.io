I"‡G<p>The <a href="https://www.tensorflow.org/guide/keras/save_and_serialize">Keras/Tensorflow Python API</a> allows simple and easy model saving, loading and inference of trained models. But performing the same operations with C++ is somehow more complicated. This article will describe how to load a <code class="language-plaintext highlighter-rouge">SavedModel</code> with C++ for inference operations. 
<!--more--></p>

<p>Using Python and <em>Tensorflow 2.X</em>, it has become super simple to <em>save</em> and <em>load</em> a model:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Saving a Tensorflow model
</span><span class="n">model</span> <span class="o">=</span> <span class="p">...</span>  <span class="c1"># Training
</span><span class="n">model</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">'path/to/model'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>

<span class="c1"># loading a Tensorflow model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">load_model</span><span class="p">(</span><span class="s">'path/to/model'</span><span class="p">)</span>

<span class="c1"># perform inference
</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="tensorflow-subclassed-model-with-multiple-inputs-and-outputs">Tensorflow Subclassed Model with multiple inputs and outputs</h4>
<p>Even when the model architectures has multiple input and outputs the code remains very <em>pythonic</em>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Tensorflow subclassed model with several inputs
</span><span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">):</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MyModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">input1</span><span class="p">,</span> <span class="n">input2</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">...</span>
    <span class="p">...</span>
    <span class="p">...</span>
    <span class="k">return</span> <span class="n">bbox_proposals</span><span class="p">,</span> <span class="n">probabilities</span>
</code></pre></div></div>

<p>We can simply call the model object directly to perform the inference. This will invoke the <code class="language-plaintext highlighter-rouge">MyModel</code>â€™s <code class="language-plaintext highlighter-rouge">call(inputs, training=False, mask=None)</code> function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">load_model</span><span class="p">(</span><span class="s">'path/to/model'</span><span class="p">)</span>

<span class="c1"># perform inference
</span><span class="n">bbox_proposals</span><span class="p">,</span> <span class="n">probabilities</span> <span class="o">=</span> <span class="n">model</span><span class="p">([</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">])</span>
</code></pre></div></div>

<h4 id="using-the-tensorflow-c-api">Using the Tensorflow C++ API</h4>

<p>However, using the Tensorflow C++ API to perform inference with a Kerasâ€™ <code class="language-plaintext highlighter-rouge">SavedModel</code> is <a href="https://www.tensorflow.org/guide/saved_model#load_a_savedmodel_in_c">not very well documented</a>. The following snippets provide a small walk through for loading and inference of the Tensorflow Model with C++.</p>

<p>As described in the <a href="https://www.tensorflow.org/guide/saved_model#load_a_savedmodel_in_c">official guide</a>, itâ€™s recommended to use <code class="language-plaintext highlighter-rouge">tensorflow::SavedModelBundle</code> which contains the MetaGraphDef and the Tensorflow session.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tensorflow</span><span class="o">::</span><span class="n">SessionOptions</span> <span class="n">session_options_</span><span class="p">;</span>
<span class="n">tensorflow</span><span class="o">::</span><span class="n">RunOptions</span> <span class="n">run_options_</span><span class="p">;</span>
<span class="n">tensorflow</span><span class="o">::</span><span class="n">SavedModelBundle</span> <span class="n">model_</span><span class="p">;</span>

<span class="k">auto</span> <span class="n">status</span> <span class="o">=</span> <span class="n">tensorflow</span><span class="o">::</span><span class="n">LoadSavedModel</span><span class="p">(</span><span class="n">session_options_</span><span class="p">,</span>
                                         <span class="n">run_options_</span><span class="p">,</span>
                                         <span class="n">path_to_model_</span><span class="p">,</span>
                                         <span class="p">{</span><span class="n">tensorflow</span><span class="o">::</span><span class="n">kSavedModelTagServe</span><span class="p">},</span>
                                         <span class="o">&amp;</span><span class="n">model_</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">status</span><span class="p">.</span><span class="n">ok</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">"Failed to load model: "</span> <span class="o">&lt;&lt;</span> <span class="n">status</span><span class="p">;</span>
<span class="k">return</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>
<p>So far, so good. We successfully loaded the model. Now, comes the tricky part.
The model that we consider is still the subclassed model defined above with two inputs and two outputs. In order to feed the model successfully during the inference we need to know the <strong>input node names</strong> and also the <strong>output node names</strong> of the computational graph. Usually, a node name in Tensorflow can be defined with the parameter <code class="language-plaintext highlighter-rouge">name="node_name"</code>. But in our case the inputs nor the outputs where tagged like that. Furthermore, the documentation does not provide any information to solve that issue.</p>

<p>Hence, we can investigate the loaded model by using the signature map of the model.</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">auto</span> <span class="n">sig_map</span> <span class="o">=</span> <span class="n">model_</span><span class="p">.</span><span class="n">GetSignatures</span><span class="p">();</span>
<span class="k">auto</span> <span class="n">model_def</span> <span class="o">=</span> <span class="n">sig_map</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="s">"serving_default"</span><span class="p">);</span>

<span class="n">printf</span><span class="p">(</span><span class="s">"Model Signature"</span><span class="p">);</span>
<span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="k">const</span><span class="o">&amp;</span> <span class="n">p</span> <span class="o">:</span> <span class="n">sig_map</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"key: %s"</span><span class="p">,</span> <span class="n">p</span><span class="p">.</span><span class="n">first</span><span class="p">.</span><span class="n">c_str</span><span class="p">());</span>
<span class="p">}</span>

<span class="n">printf</span><span class="p">(</span><span class="s">"Model Input Nodes"</span><span class="p">);</span>
<span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="k">const</span><span class="o">&amp;</span> <span class="n">p</span> <span class="o">:</span> <span class="n">model_def</span><span class="p">.</span><span class="n">inputs</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"key: %s value: %s"</span><span class="p">,</span> <span class="n">p</span><span class="p">.</span><span class="n">first</span><span class="p">.</span><span class="n">c_str</span><span class="p">(),</span> <span class="n">p</span><span class="p">.</span><span class="n">second</span><span class="p">.</span><span class="n">name</span><span class="p">().</span><span class="n">c_str</span><span class="p">());</span>
<span class="p">}</span>

<span class="n">printf</span><span class="p">(</span><span class="s">"Model Output Nodes"</span><span class="p">);</span>
<span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="k">const</span><span class="o">&amp;</span> <span class="n">p</span> <span class="o">:</span> <span class="n">model_def</span><span class="p">.</span><span class="n">outputs</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"key: %s value: %s"</span><span class="p">,</span> <span class="n">p</span><span class="p">.</span><span class="n">first</span><span class="p">.</span><span class="n">c_str</span><span class="p">(),</span> <span class="n">p</span><span class="p">.</span><span class="n">second</span><span class="p">.</span><span class="n">name</span><span class="p">().</span><span class="n">c_str</span><span class="p">());</span>
<span class="p">}</span>
</code></pre></div></div>
<p>Which will print something like</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model Signature
key: __saved_model_init_op
key: serving_default
Model Input Nodes
key: input_1 value: serving_default_input_1:0
key: input_2 value: serving_default_input_2:0
Model Output Nodes
key: output_1 value: StatefulPartitionedCall:0
key: output_2 value: StatefulPartitionedCall:1
</code></pre></div></div>

<p>Great, our input nodes are called</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">serving_default_input_1:0</code></li>
  <li><code class="language-plaintext highlighter-rouge">serving_default_input_2:0</code></li>
</ul>

<p>and the output nodes</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">StatefulPartitionedCall:0</code></li>
  <li><code class="language-plaintext highlighter-rouge">StatefulPartitionedCall:1</code>.</li>
</ul>

<p>We can use that information to perform the inference session by getting the node names:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">input_name_1</span> <span class="o">=</span> <span class="n">model_def</span><span class="p">.</span><span class="n">inputs</span><span class="p">().</span><span class="n">at</span><span class="p">(</span><span class="s">"input_1"</span><span class="p">).</span><span class="n">name</span><span class="p">();</span>
<span class="n">input_name_2</span> <span class="o">=</span> <span class="n">model_def</span><span class="p">.</span><span class="n">inputs</span><span class="p">().</span><span class="n">at</span><span class="p">(</span><span class="s">"input_2"</span><span class="p">).</span><span class="n">name</span><span class="p">();</span>

<span class="n">output_name_bbox_proposals</span> <span class="o">=</span> <span class="n">model_def</span><span class="p">.</span><span class="n">outputs</span><span class="p">().</span><span class="n">at</span><span class="p">(</span><span class="s">"output_1"</span><span class="p">).</span><span class="n">name</span><span class="p">();</span>
<span class="n">output_name_probabilities</span> <span class="o">=</span> <span class="n">model_def</span><span class="p">.</span><span class="n">outputs</span><span class="p">().</span><span class="n">at</span><span class="p">(</span><span class="s">"output_2"</span><span class="p">).</span><span class="n">name</span><span class="p">();</span>
</code></pre></div></div>

<p>And then finally run the model within the session.</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TFTensor</span><span class="o">&gt;</span> <span class="n">inputTensor_1</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TFTensor</span><span class="o">&gt;</span> <span class="n">inputTensor_2</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TFTensor</span><span class="o">&gt;</span> <span class="n">bbox_output</span><span class="p">;</span>

<span class="c1">// fill the input tensors with data</span>

<span class="n">tensorflow</span><span class="o">::</span><span class="n">Status</span> <span class="n">status</span><span class="p">;</span>
<span class="n">status</span> <span class="o">=</span> <span class="n">model_</span><span class="p">.</span><span class="n">session</span><span class="o">-&gt;</span><span class="n">Run</span><span class="p">({</span> <span class="p">{</span><span class="n">input_name_1</span><span class="p">,</span> <span class="n">inputTensor_1</span><span class="p">},</span>
                               <span class="p">{</span><span class="n">input_name_2</span><span class="p">,</span> <span class="n">inputTensor_2</span><span class="p">}</span> <span class="p">},</span>
                               <span class="p">{</span><span class="n">output_name_bbox_proposals</span><span class="p">},</span> <span class="p">{},</span> <span class="o">&amp;</span><span class="n">bbox_output</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">status</span><span class="p">.</span><span class="n">ok</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">"Inference failed: "</span> <span class="o">&lt;&lt;</span> <span class="n">status</span><span class="p">;</span>
    <span class="k">return</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Note, that on a GPU device we can use similar to the Python API the <code class="language-plaintext highlighter-rouge">set_allow_growth</code> variable for proper GPU RAM allocation.</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">session_options_</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">mutable_gpu_options</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">set_allow_growth</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
</code></pre></div></div>

<h4 id="troubleshooting">Troubleshooting</h4>

<p>In case you are encountering the following error during runtime,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>undefined symbol: _ZNK6google8protobuf8internal12MapFieldBase24SyncMapWithRepeatedFieldEv
</code></pre></div></div>

<p>you need to make sure that you link the C++ code against Googleâ€™s <strong>Protobuf library</strong>. These libs can be installed with</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> libprotobuf-dev
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> protobuf-compiler
</code></pre></div></div>

<h4 id="wrap-up">Wrap-up</h4>

<ul>
  <li>We should load a Kerasâ€™ <code class="language-plaintext highlighter-rouge">SavedModel</code> with <code class="language-plaintext highlighter-rouge">tensorflow::LoadSavedModel</code> in C++</li>
  <li>The names of input and output nodes of a subclassed Tensorflow model are not always uniquely defined</li>
  <li>We can look up the node names by examining the signature map of the model</li>
  <li>The inference of the model is then performed by call the modelâ€™s session</li>
</ul>
:ET